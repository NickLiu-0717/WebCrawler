// package crawl

// import (
// 	"fmt"
// 	"net/url"

// 	"github.com/NickLiu-0717/crawler/config"
// 	"github.com/NickLiu-0717/crawler/service"
// )

// type CrawlConfig struct {
// 	Config *config.Config
// }

// func (cfg *CrawlConfig) CrawlPage(rawCurrentURL string, depth int) {

// 	cfg.Config.ConcurrencyControl <- struct{}{}
// 	defer func() {
// 		<-cfg.Config.ConcurrencyControl
// 		cfg.Config.Wg.Done()
// 	}()

// 	if depth > cfg.Config.MaxDepth {
// 		return
// 	}

// 	cfg.Config.Mu.Lock()
// 	if len(cfg.Config.Pages) >= cfg.Config.MaxPages {
// 		cfg.Config.Mu.Unlock()
// 		return
// 	}
// 	cfg.Config.Mu.Unlock()

// 	parsedCurrentURL, err := url.Parse(rawCurrentURL)
// 	if err != nil {
// 		fmt.Printf("Error - couldn't parse current URL: %v\n", err)
// 		return
// 	}

// 	if cfg.Config.BaseURL.Hostname() != parsedCurrentURL.Hostname() {
// 		return
// 	}
// 	//Normalize the path
// 	norCurrentURL, err := NormalizeURL(parsedCurrentURL.String())
// 	if err != nil {
// 		fmt.Printf("Error - couldn't normalize current URL: %v\n", err)
// 		return
// 	}
// 	//Test if the path has robots.txt restriction
// 	if cfg.Config.RobotGroup != nil {
// 		if !cfg.Config.RobotGroup.Test(parsedCurrentURL.Path) {
// 			fmt.Printf("URL %s is not allowed to crawl\n", parsedCurrentURL.String())
// 			return
// 		}
// 	}

// 	cfg.Config.Mu.Lock()
// 	if _, ok := cfg.Config.Pages[norCurrentURL]; ok {
// 		cfg.Config.Pages[norCurrentURL]++
// 		cfg.Config.Mu.Unlock()
// 		return
// 	}
// 	cfg.Config.Pages[norCurrentURL] = 1
// 	cfg.Config.Mu.Unlock()

// 	// fmt.Printf("Start crawling %s\n", rawCurrentURL)
// 	//Random Sleep to simulate human behavior
// 	config.RandomSleep(1000, 2000)
// 	html, err := GetHTML(parsedCurrentURL.String())
// 	if err != nil {
// 		fmt.Printf("Error - couldn't get HTML: %v\n", err)
// 		return
// 	}
// 	//Check if the path is article or not, if article, extract title and content then return, no more crawling
// 	if service.CheckArticle(parsedCurrentURL.String()) {
// 		gottitle, gotcontent, gotpubAt, err := service.ExtractArticles(html)
// 		if err != nil {
// 			fmt.Printf("Error - couldn't extract title and content from HTML: %v\n", err)
// 			return
// 		}

// 		gotcatagory, err := service.ClassifyArticle(gottitle)
// 		if err != nil {
// 			fmt.Printf("couldn't classify article by title: %v\n", err)
// 			return
// 		}
// 		err = cfg.CreateArticles(norCurrentURL, gottitle, gotcontent, gotcatagory, gotpubAt.UTC())
// 		if err != nil {
// 			// fmt.Printf("couldn't create article to database: %v\n", err)
// 			return
// 		}
// 		return
// 	}

// 	URLs, err := GetURLsFromHTML(html, parsedCurrentURL.String())
// 	if err != nil {
// 		fmt.Printf("Error - couldn't get URL from HTML: %v\n", err)
// 		return
// 	}
// 	for _, nextURL := range URLs {
// 		cfg.Config.Wg.Add(1)
// 		go cfg.CrawlPage(nextURL, depth+1)
// 	}

// }
